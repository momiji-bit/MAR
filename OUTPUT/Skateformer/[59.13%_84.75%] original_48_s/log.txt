[ Tue Jan  7 23:38:41 2025 ] using warm up, epoch: 10
[ Tue Jan  7 23:38:42 2025 ] Parameters:
{'work_dir': './output/original_48_s/', 'model_saved_name': './output/original_48_s/runs', 'config': './config/SkateFormer_j_s_NEW.yaml', 'weights': None, 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ma52_NEW.Feeder', 'num_worker': 6, 'train_feeder_args': {'data_path': 'new_ma52/json', 'label_path': 'train', 'data_type': 'j', 'repeat': 10, 'p': 0.3, 'debug': False, 'partition': True}, 'test_feeder_args': {'data_path': 'new_ma52/json', 'label_path': 'val', 'data_type': 'j', 'repeat': 1, 'partition': True}, 'model': 'model.SkateFormer.SkateFormer_', 'model_args': {'in_channels': 2, 'num_classes': 52, 'num_people': 1, 'num_points': 48, 'kernel_size': 7, 'num_heads': 16, 'attn_drop': 0.5, 'head_drop': 0.0, 'rel': True, 'drop_path': 0.3, 'type_1_size': [8, 12], 'type_2_size': [8, 24], 'type_3_size': [8, 12], 'type_4_size': [8, 24], 'mlp_ratio': 0.8, 'index_t': True}, 'ignore_weights': [], 'base_lr': 0.0005, 'min_lr': 1e-05, 'warmup_lr': 1e-07, 'warmup_prefix': False, 'warm_up_epoch': 10, 'grad_clip': True, 'grad_max': 1.0, 'device': [0, 1, 2, 3], 'optimizer': 'AdamW', 'lr_scheduler': 'cosine', 'nesterov': True, 'batch_size': 448, 'test_batch_size': 448, 'start_epoch': 0, 'num_epoch': 100, 'weight_decay': 0.01, 'lr_ratio': 0.001, 'lr_decay_rate': 0.1, 'loss_type': 'LSCE'}

[ Tue Jan  7 23:38:42 2025 ] # Parameters: 2151804
[ Tue Jan  7 23:38:42 2025 ] Training epoch: 1
[ Tue Jan  7 23:40:27 2025 ] 	Mean training loss: 3.7056.  Mean training acc: 8.86%.
[ Tue Jan  7 23:40:27 2025 ] 	Learning Rate: 0.00004989
[ Tue Jan  7 23:40:27 2025 ] 	Time consumption: [Data]04%, [Network]96%
[ Tue Jan  7 23:40:27 2025 ] Eval epoch: 1
[ Tue Jan  7 23:40:32 2025 ] 	Mean test loss of 13 batches: 4.506715224339412.
[ Tue Jan  7 23:40:32 2025 ] 	Top1: 2.86%
[ Tue Jan  7 23:40:32 2025 ] 	Top5: 10.15%
[ Tue Jan  7 23:40:32 2025 ] Training epoch: 2
[ Tue Jan  7 23:42:14 2025 ] 	Mean training loss: 3.2910.  Mean training acc: 14.58%.
[ Tue Jan  7 23:42:14 2025 ] 	Learning Rate: 0.00009988
[ Tue Jan  7 23:42:14 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Tue Jan  7 23:42:14 2025 ] Eval epoch: 2
[ Tue Jan  7 23:42:19 2025 ] 	Mean test loss of 13 batches: 3.6911245492788463.
[ Tue Jan  7 23:42:19 2025 ] 	Top1: 8.31%
[ Tue Jan  7 23:42:19 2025 ] 	Top5: 33.94%
[ Tue Jan  7 23:42:19 2025 ] Training epoch: 3
[ Tue Jan  7 23:43:58 2025 ] 	Mean training loss: 3.0670.  Mean training acc: 19.35%.
[ Tue Jan  7 23:43:58 2025 ] 	Learning Rate: 0.00014987
[ Tue Jan  7 23:43:58 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Tue Jan  7 23:43:58 2025 ] Eval epoch: 3
[ Tue Jan  7 23:44:03 2025 ] 	Mean test loss of 13 batches: 2.9205862375406118.
[ Tue Jan  7 23:44:03 2025 ] 	Top1: 22.18%
[ Tue Jan  7 23:44:03 2025 ] 	Top5: 63.32%
[ Tue Jan  7 23:44:03 2025 ] Training epoch: 4
[ Tue Jan  7 23:45:42 2025 ] 	Mean training loss: 2.8734.  Mean training acc: 24.95%.
[ Tue Jan  7 23:45:42 2025 ] 	Learning Rate: 0.00019986
[ Tue Jan  7 23:45:42 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Tue Jan  7 23:45:42 2025 ] Eval epoch: 4
[ Tue Jan  7 23:45:48 2025 ] 	Mean test loss of 13 batches: 2.883316296797532.
[ Tue Jan  7 23:45:48 2025 ] 	Top1: 27.18%
[ Tue Jan  7 23:45:48 2025 ] 	Top5: 64.02%
[ Tue Jan  7 23:45:48 2025 ] Training epoch: 5
[ Tue Jan  7 23:47:28 2025 ] 	Mean training loss: 2.7194.  Mean training acc: 30.13%.
[ Tue Jan  7 23:47:28 2025 ] 	Learning Rate: 0.00024985
[ Tue Jan  7 23:47:28 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Tue Jan  7 23:47:28 2025 ] Eval epoch: 5
[ Tue Jan  7 23:47:34 2025 ] 	Mean test loss of 13 batches: 2.6889208463522105.
[ Tue Jan  7 23:47:34 2025 ] 	Top1: 32.56%
[ Tue Jan  7 23:47:34 2025 ] 	Top5: 71.09%
[ Tue Jan  7 23:47:34 2025 ] Training epoch: 6
[ Tue Jan  7 23:49:12 2025 ] 	Mean training loss: 2.5887.  Mean training acc: 34.71%.
[ Tue Jan  7 23:49:12 2025 ] 	Learning Rate: 0.00029984
[ Tue Jan  7 23:49:12 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Tue Jan  7 23:49:12 2025 ] Eval epoch: 6
[ Tue Jan  7 23:49:17 2025 ] 	Mean test loss of 13 batches: 2.778285631766686.
[ Tue Jan  7 23:49:17 2025 ] 	Top1: 32.51%
[ Tue Jan  7 23:49:17 2025 ] 	Top5: 68.37%
[ Tue Jan  7 23:49:17 2025 ] Training epoch: 7
[ Tue Jan  7 23:50:55 2025 ] 	Mean training loss: 2.4593.  Mean training acc: 39.45%.
[ Tue Jan  7 23:50:55 2025 ] 	Learning Rate: 0.00034983
[ Tue Jan  7 23:50:55 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Tue Jan  7 23:50:55 2025 ] Eval epoch: 7
[ Tue Jan  7 23:51:00 2025 ] 	Mean test loss of 13 batches: 2.395733191416814.
[ Tue Jan  7 23:51:00 2025 ] 	Top1: 43.11%
[ Tue Jan  7 23:51:00 2025 ] 	Top5: 78.25%
[ Tue Jan  7 23:51:00 2025 ] Training epoch: 8
[ Tue Jan  7 23:52:38 2025 ] 	Mean training loss: 2.3466.  Mean training acc: 43.18%.
[ Tue Jan  7 23:52:38 2025 ] 	Learning Rate: 0.00039982
[ Tue Jan  7 23:52:38 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Tue Jan  7 23:52:38 2025 ] Eval epoch: 8
[ Tue Jan  7 23:52:43 2025 ] 	Mean test loss of 13 batches: 2.311198381277231.
[ Tue Jan  7 23:52:43 2025 ] 	Top1: 46.78%
[ Tue Jan  7 23:52:43 2025 ] 	Top5: 81.17%
[ Tue Jan  7 23:52:43 2025 ] Training epoch: 9
[ Tue Jan  7 23:54:21 2025 ] 	Mean training loss: 2.2467.  Mean training acc: 46.68%.
[ Tue Jan  7 23:54:21 2025 ] 	Learning Rate: 0.00044981
[ Tue Jan  7 23:54:21 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Tue Jan  7 23:54:21 2025 ] Eval epoch: 9
[ Tue Jan  7 23:54:26 2025 ] 	Mean test loss of 13 batches: 2.2572885293226976.
[ Tue Jan  7 23:54:26 2025 ] 	Top1: 48.59%
[ Tue Jan  7 23:54:26 2025 ] 	Top5: 82.04%
[ Tue Jan  7 23:54:26 2025 ] Training epoch: 10
[ Tue Jan  7 23:56:04 2025 ] 	Mean training loss: 2.1537.  Mean training acc: 49.83%.
[ Tue Jan  7 23:56:04 2025 ] 	Learning Rate: 0.00049980
[ Tue Jan  7 23:56:04 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Tue Jan  7 23:56:04 2025 ] Eval epoch: 10
[ Tue Jan  7 23:56:09 2025 ] 	Mean test loss of 13 batches: 2.282860590861394.
[ Tue Jan  7 23:56:09 2025 ] 	Top1: 47.62%
[ Tue Jan  7 23:56:09 2025 ] 	Top5: 81.63%
[ Tue Jan  7 23:56:09 2025 ] Training epoch: 11
[ Tue Jan  7 23:57:48 2025 ] 	Mean training loss: 2.0710.  Mean training acc: 52.55%.
[ Tue Jan  7 23:57:48 2025 ] 	Learning Rate: 0.00048553
[ Tue Jan  7 23:57:48 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Tue Jan  7 23:57:48 2025 ] Eval epoch: 11
[ Tue Jan  7 23:57:53 2025 ] 	Mean test loss of 13 batches: 2.134392637472886.
[ Tue Jan  7 23:57:53 2025 ] 	Top1: 52.38%
[ Tue Jan  7 23:57:53 2025 ] 	Top5: 85.05%
[ Tue Jan  7 23:57:53 2025 ] Training epoch: 12
[ Tue Jan  7 23:59:32 2025 ] 	Mean training loss: 1.9847.  Mean training acc: 55.61%.
[ Tue Jan  7 23:59:32 2025 ] 	Learning Rate: 0.00048281
[ Tue Jan  7 23:59:32 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Tue Jan  7 23:59:32 2025 ] Eval epoch: 12
[ Tue Jan  7 23:59:37 2025 ] 	Mean test loss of 13 batches: 2.22059205862192.
[ Tue Jan  7 23:59:37 2025 ] 	Top1: 50.30%
[ Tue Jan  7 23:59:37 2025 ] 	Top5: 83.30%
[ Tue Jan  7 23:59:37 2025 ] Training epoch: 13
[ Wed Jan  8 00:01:15 2025 ] 	Mean training loss: 1.9087.  Mean training acc: 58.27%.
[ Wed Jan  8 00:01:15 2025 ] 	Learning Rate: 0.00047986
[ Wed Jan  8 00:01:15 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 00:01:15 2025 ] Eval epoch: 13
[ Wed Jan  8 00:01:20 2025 ] 	Mean test loss of 13 batches: 2.141216415625352.
[ Wed Jan  8 00:01:20 2025 ] 	Top1: 52.47%
[ Wed Jan  8 00:01:20 2025 ] 	Top5: 84.84%
[ Wed Jan  8 00:01:20 2025 ] Training epoch: 14
[ Wed Jan  8 00:02:58 2025 ] 	Mean training loss: 1.8381.  Mean training acc: 61.01%.
[ Wed Jan  8 00:02:58 2025 ] 	Learning Rate: 0.00047670
[ Wed Jan  8 00:02:58 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 00:02:58 2025 ] Eval epoch: 14
[ Wed Jan  8 00:03:03 2025 ] 	Mean test loss of 13 batches: 2.12710702419281.
[ Wed Jan  8 00:03:03 2025 ] 	Top1: 54.33%
[ Wed Jan  8 00:03:03 2025 ] 	Top5: 85.75%
[ Wed Jan  8 00:03:03 2025 ] Training epoch: 15
[ Wed Jan  8 00:04:41 2025 ] 	Mean training loss: 1.7752.  Mean training acc: 63.36%.
[ Wed Jan  8 00:04:41 2025 ] 	Learning Rate: 0.00047331
[ Wed Jan  8 00:04:41 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 00:04:41 2025 ] Eval epoch: 15
[ Wed Jan  8 00:04:46 2025 ] 	Mean test loss of 13 batches: 2.070561326467074.
[ Wed Jan  8 00:04:46 2025 ] 	Top1: 56.44%
[ Wed Jan  8 00:04:46 2025 ] 	Top5: 85.86%
[ Wed Jan  8 00:04:46 2025 ] Training epoch: 16
[ Wed Jan  8 00:06:25 2025 ] 	Mean training loss: 1.7160.  Mean training acc: 65.48%.
[ Wed Jan  8 00:06:25 2025 ] 	Learning Rate: 0.00046971
[ Wed Jan  8 00:06:25 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 00:06:25 2025 ] Eval epoch: 16
[ Wed Jan  8 00:06:30 2025 ] 	Mean test loss of 13 batches: 2.076759879405682.
[ Wed Jan  8 00:06:30 2025 ] 	Top1: 56.80%
[ Wed Jan  8 00:06:30 2025 ] 	Top5: 87.27%
[ Wed Jan  8 00:06:30 2025 ] Training epoch: 17
[ Wed Jan  8 00:08:09 2025 ] 	Mean training loss: 1.6677.  Mean training acc: 67.48%.
[ Wed Jan  8 00:08:09 2025 ] 	Learning Rate: 0.00046590
[ Wed Jan  8 00:08:09 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:08:09 2025 ] Eval epoch: 17
[ Wed Jan  8 00:08:15 2025 ] 	Mean test loss of 13 batches: 2.1211815430567813.
[ Wed Jan  8 00:08:15 2025 ] 	Top1: 56.25%
[ Wed Jan  8 00:08:15 2025 ] 	Top5: 86.52%
[ Wed Jan  8 00:08:15 2025 ] Training epoch: 18
[ Wed Jan  8 00:09:54 2025 ] 	Mean training loss: 1.6225.  Mean training acc: 69.18%.
[ Wed Jan  8 00:09:54 2025 ] 	Learning Rate: 0.00046188
[ Wed Jan  8 00:09:54 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:09:54 2025 ] Eval epoch: 18
[ Wed Jan  8 00:09:59 2025 ] 	Mean test loss of 13 batches: 2.085412887426523.
[ Wed Jan  8 00:09:59 2025 ] 	Top1: 56.30%
[ Wed Jan  8 00:09:59 2025 ] 	Top5: 86.70%
[ Wed Jan  8 00:09:59 2025 ] Training epoch: 19
[ Wed Jan  8 00:11:38 2025 ] 	Mean training loss: 1.5833.  Mean training acc: 70.79%.
[ Wed Jan  8 00:11:38 2025 ] 	Learning Rate: 0.00045765
[ Wed Jan  8 00:11:38 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:11:38 2025 ] Eval epoch: 19
[ Wed Jan  8 00:11:44 2025 ] 	Mean test loss of 13 batches: 2.1188450318116407.
[ Wed Jan  8 00:11:44 2025 ] 	Top1: 56.53%
[ Wed Jan  8 00:11:44 2025 ] 	Top5: 86.22%
[ Wed Jan  8 00:11:44 2025 ] Training epoch: 20
[ Wed Jan  8 00:13:23 2025 ] 	Mean training loss: 1.5396.  Mean training acc: 72.39%.
[ Wed Jan  8 00:13:23 2025 ] 	Learning Rate: 0.00045323
[ Wed Jan  8 00:13:23 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:13:23 2025 ] Eval epoch: 20
[ Wed Jan  8 00:13:29 2025 ] 	Mean test loss of 13 batches: 2.173974743256202.
[ Wed Jan  8 00:13:29 2025 ] 	Top1: 55.69%
[ Wed Jan  8 00:13:29 2025 ] 	Top5: 85.61%
[ Wed Jan  8 00:13:29 2025 ] Training epoch: 21
[ Wed Jan  8 00:15:08 2025 ] 	Mean training loss: 1.5030.  Mean training acc: 73.85%.
[ Wed Jan  8 00:15:08 2025 ] 	Learning Rate: 0.00044861
[ Wed Jan  8 00:15:08 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:15:08 2025 ] Eval epoch: 21
[ Wed Jan  8 00:15:14 2025 ] 	Mean test loss of 13 batches: 2.108464231857887.
[ Wed Jan  8 00:15:14 2025 ] 	Top1: 57.61%
[ Wed Jan  8 00:15:14 2025 ] 	Top5: 86.79%
[ Wed Jan  8 00:15:14 2025 ] Training epoch: 22
[ Wed Jan  8 00:16:53 2025 ] 	Mean training loss: 1.4736.  Mean training acc: 75.10%.
[ Wed Jan  8 00:16:53 2025 ] 	Learning Rate: 0.00044380
[ Wed Jan  8 00:16:53 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:16:53 2025 ] Eval epoch: 22
[ Wed Jan  8 00:16:59 2025 ] 	Mean test loss of 13 batches: 2.111683497062096.
[ Wed Jan  8 00:16:59 2025 ] 	Top1: 58.16%
[ Wed Jan  8 00:16:59 2025 ] 	Top5: 86.73%
[ Wed Jan  8 00:16:59 2025 ] Training epoch: 23
[ Wed Jan  8 00:18:38 2025 ] 	Mean training loss: 1.4496.  Mean training acc: 75.95%.
[ Wed Jan  8 00:18:38 2025 ] 	Learning Rate: 0.00043880
[ Wed Jan  8 00:18:38 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:18:38 2025 ] Eval epoch: 23
[ Wed Jan  8 00:18:44 2025 ] 	Mean test loss of 13 batches: 2.170706730622512.
[ Wed Jan  8 00:18:44 2025 ] 	Top1: 57.11%
[ Wed Jan  8 00:18:44 2025 ] 	Top5: 85.66%
[ Wed Jan  8 00:18:44 2025 ] Training epoch: 24
[ Wed Jan  8 00:20:23 2025 ] 	Mean training loss: 1.4270.  Mean training acc: 76.75%.
[ Wed Jan  8 00:20:23 2025 ] 	Learning Rate: 0.00043362
[ Wed Jan  8 00:20:23 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:20:23 2025 ] Eval epoch: 24
[ Wed Jan  8 00:20:29 2025 ] 	Mean test loss of 13 batches: 2.156585922608009.
[ Wed Jan  8 00:20:29 2025 ] 	Top1: 57.14%
[ Wed Jan  8 00:20:29 2025 ] 	Top5: 85.71%
[ Wed Jan  8 00:20:29 2025 ] Training epoch: 25
[ Wed Jan  8 00:22:08 2025 ] 	Mean training loss: 1.3931.  Mean training acc: 78.19%.
[ Wed Jan  8 00:22:08 2025 ] 	Learning Rate: 0.00042826
[ Wed Jan  8 00:22:08 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:22:08 2025 ] Eval epoch: 25
[ Wed Jan  8 00:22:13 2025 ] 	Mean test loss of 13 batches: 2.1586553225150475.
[ Wed Jan  8 00:22:13 2025 ] 	Top1: 57.64%
[ Wed Jan  8 00:22:13 2025 ] 	Top5: 86.27%
[ Wed Jan  8 00:22:13 2025 ] Training epoch: 26
[ Wed Jan  8 00:23:52 2025 ] 	Mean training loss: 1.3748.  Mean training acc: 78.92%.
[ Wed Jan  8 00:23:52 2025 ] 	Learning Rate: 0.00042274
[ Wed Jan  8 00:23:52 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:23:53 2025 ] Eval epoch: 26
[ Wed Jan  8 00:23:58 2025 ] 	Mean test loss of 13 batches: 2.16704391516172.
[ Wed Jan  8 00:23:58 2025 ] 	Top1: 57.43%
[ Wed Jan  8 00:23:58 2025 ] 	Top5: 86.13%
[ Wed Jan  8 00:23:58 2025 ] Training epoch: 27
[ Wed Jan  8 00:25:37 2025 ] 	Mean training loss: 1.3499.  Mean training acc: 79.94%.
[ Wed Jan  8 00:25:37 2025 ] 	Learning Rate: 0.00041704
[ Wed Jan  8 00:25:37 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 00:25:37 2025 ] Eval epoch: 27
[ Wed Jan  8 00:25:43 2025 ] 	Mean test loss of 13 batches: 2.161832204231849.
[ Wed Jan  8 00:25:43 2025 ] 	Top1: 58.00%
[ Wed Jan  8 00:25:43 2025 ] 	Top5: 85.48%
[ Wed Jan  8 00:25:43 2025 ] Training epoch: 28
[ Wed Jan  8 00:27:22 2025 ] 	Mean training loss: 1.3337.  Mean training acc: 80.46%.
[ Wed Jan  8 00:27:22 2025 ] 	Learning Rate: 0.00041119
[ Wed Jan  8 00:27:22 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 00:27:22 2025 ] Eval epoch: 28
[ Wed Jan  8 00:27:28 2025 ] 	Mean test loss of 13 batches: 2.147830605506897.
[ Wed Jan  8 00:27:28 2025 ] 	Top1: 57.66%
[ Wed Jan  8 00:27:28 2025 ] 	Top5: 85.89%
[ Wed Jan  8 00:27:28 2025 ] Training epoch: 29
[ Wed Jan  8 00:29:07 2025 ] 	Mean training loss: 1.3194.  Mean training acc: 81.06%.
[ Wed Jan  8 00:29:07 2025 ] 	Learning Rate: 0.00040519
[ Wed Jan  8 00:29:07 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:29:07 2025 ] Eval epoch: 29
[ Wed Jan  8 00:29:13 2025 ] 	Mean test loss of 13 batches: 2.157440213056711.
[ Wed Jan  8 00:29:13 2025 ] 	Top1: 58.04%
[ Wed Jan  8 00:29:13 2025 ] 	Top5: 85.55%
[ Wed Jan  8 00:29:13 2025 ] Training epoch: 30
[ Wed Jan  8 00:30:52 2025 ] 	Mean training loss: 1.2963.  Mean training acc: 82.08%.
[ Wed Jan  8 00:30:52 2025 ] 	Learning Rate: 0.00039903
[ Wed Jan  8 00:30:52 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:30:52 2025 ] Eval epoch: 30
[ Wed Jan  8 00:30:58 2025 ] 	Mean test loss of 13 batches: 2.1671509009141188.
[ Wed Jan  8 00:30:58 2025 ] 	Top1: 57.43%
[ Wed Jan  8 00:30:58 2025 ] 	Top5: 85.34%
[ Wed Jan  8 00:30:58 2025 ] Training epoch: 31
[ Wed Jan  8 00:32:37 2025 ] 	Mean training loss: 1.2739.  Mean training acc: 82.80%.
[ Wed Jan  8 00:32:37 2025 ] 	Learning Rate: 0.00039274
[ Wed Jan  8 00:32:37 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 00:32:37 2025 ] Eval epoch: 31
[ Wed Jan  8 00:32:42 2025 ] 	Mean test loss of 13 batches: 2.1736571421990027.
[ Wed Jan  8 00:32:42 2025 ] 	Top1: 58.06%
[ Wed Jan  8 00:32:43 2025 ] 	Top5: 85.88%
[ Wed Jan  8 00:32:43 2025 ] Training epoch: 32
[ Wed Jan  8 00:34:22 2025 ] 	Mean training loss: 1.2730.  Mean training acc: 82.75%.
[ Wed Jan  8 00:34:22 2025 ] 	Learning Rate: 0.00038630
[ Wed Jan  8 00:34:22 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:34:22 2025 ] Eval epoch: 32
[ Wed Jan  8 00:34:27 2025 ] 	Mean test loss of 13 batches: 2.147061127882737.
[ Wed Jan  8 00:34:27 2025 ] 	Top1: 58.88%
[ Wed Jan  8 00:34:27 2025 ] 	Top5: 85.48%
[ Wed Jan  8 00:34:27 2025 ] Training epoch: 33
[ Wed Jan  8 00:36:07 2025 ] 	Mean training loss: 1.2448.  Mean training acc: 83.90%.
[ Wed Jan  8 00:36:07 2025 ] 	Learning Rate: 0.00037974
[ Wed Jan  8 00:36:07 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:36:07 2025 ] Eval epoch: 33
[ Wed Jan  8 00:36:13 2025 ] 	Mean test loss of 13 batches: 2.1660472521415124.
[ Wed Jan  8 00:36:13 2025 ] 	Top1: 58.07%
[ Wed Jan  8 00:36:13 2025 ] 	Top5: 85.68%
[ Wed Jan  8 00:36:13 2025 ] Training epoch: 34
[ Wed Jan  8 00:37:53 2025 ] 	Mean training loss: 1.2358.  Mean training acc: 84.15%.
[ Wed Jan  8 00:37:53 2025 ] 	Learning Rate: 0.00037306
[ Wed Jan  8 00:37:53 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:37:53 2025 ] Eval epoch: 34
[ Wed Jan  8 00:37:58 2025 ] 	Mean test loss of 13 batches: 2.171692518087534.
[ Wed Jan  8 00:37:58 2025 ] 	Top1: 58.27%
[ Wed Jan  8 00:37:58 2025 ] 	Top5: 85.48%
[ Wed Jan  8 00:37:58 2025 ] Training epoch: 35
[ Wed Jan  8 00:39:38 2025 ] 	Mean training loss: 1.2269.  Mean training acc: 84.50%.
[ Wed Jan  8 00:39:38 2025 ] 	Learning Rate: 0.00036625
[ Wed Jan  8 00:39:38 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:39:38 2025 ] Eval epoch: 35
[ Wed Jan  8 00:39:44 2025 ] 	Mean test loss of 13 batches: 2.180905342102051.
[ Wed Jan  8 00:39:44 2025 ] 	Top1: 57.52%
[ Wed Jan  8 00:39:44 2025 ] 	Top5: 85.00%
[ Wed Jan  8 00:39:44 2025 ] Training epoch: 36
[ Wed Jan  8 00:41:23 2025 ] 	Mean training loss: 1.2111.  Mean training acc: 85.13%.
[ Wed Jan  8 00:41:23 2025 ] 	Learning Rate: 0.00035934
[ Wed Jan  8 00:41:23 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 00:41:23 2025 ] Eval epoch: 36
[ Wed Jan  8 00:41:29 2025 ] 	Mean test loss of 13 batches: 2.1357270754300632.
[ Wed Jan  8 00:41:29 2025 ] 	Top1: 58.27%
[ Wed Jan  8 00:41:29 2025 ] 	Top5: 85.57%
[ Wed Jan  8 00:41:29 2025 ] Training epoch: 37
[ Wed Jan  8 00:43:09 2025 ] 	Mean training loss: 1.2021.  Mean training acc: 85.38%.
[ Wed Jan  8 00:43:09 2025 ] 	Learning Rate: 0.00035233
[ Wed Jan  8 00:43:09 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:43:09 2025 ] Eval epoch: 37
[ Wed Jan  8 00:43:15 2025 ] 	Mean test loss of 13 batches: 2.1332726386877208.
[ Wed Jan  8 00:43:15 2025 ] 	Top1: 58.68%
[ Wed Jan  8 00:43:15 2025 ] 	Top5: 85.57%
[ Wed Jan  8 00:43:15 2025 ] Training epoch: 38
[ Wed Jan  8 00:44:54 2025 ] 	Mean training loss: 1.1849.  Mean training acc: 85.87%.
[ Wed Jan  8 00:44:54 2025 ] 	Learning Rate: 0.00034522
[ Wed Jan  8 00:44:54 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:44:54 2025 ] Eval epoch: 38
[ Wed Jan  8 00:45:00 2025 ] 	Mean test loss of 13 batches: 2.1587891303575955.
[ Wed Jan  8 00:45:00 2025 ] 	Top1: 57.89%
[ Wed Jan  8 00:45:00 2025 ] 	Top5: 85.12%
[ Wed Jan  8 00:45:00 2025 ] Training epoch: 39
[ Wed Jan  8 00:46:39 2025 ] 	Mean training loss: 1.1796.  Mean training acc: 86.32%.
[ Wed Jan  8 00:46:39 2025 ] 	Learning Rate: 0.00033802
[ Wed Jan  8 00:46:39 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:46:39 2025 ] Eval epoch: 39
[ Wed Jan  8 00:46:44 2025 ] 	Mean test loss of 13 batches: 2.159323600622324.
[ Wed Jan  8 00:46:44 2025 ] 	Top1: 58.86%
[ Wed Jan  8 00:46:44 2025 ] 	Top5: 85.48%
[ Wed Jan  8 00:46:44 2025 ] Training epoch: 40
[ Wed Jan  8 00:48:26 2025 ] 	Mean training loss: 1.1607.  Mean training acc: 86.87%.
[ Wed Jan  8 00:48:26 2025 ] 	Learning Rate: 0.00033074
[ Wed Jan  8 00:48:26 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:48:26 2025 ] Eval epoch: 40
[ Wed Jan  8 00:48:31 2025 ] 	Mean test loss of 13 batches: 2.164875003007742.
[ Wed Jan  8 00:48:31 2025 ] 	Top1: 58.15%
[ Wed Jan  8 00:48:31 2025 ] 	Top5: 84.85%
[ Wed Jan  8 00:48:31 2025 ] Training epoch: 41
[ Wed Jan  8 00:50:10 2025 ] 	Mean training loss: 1.1594.  Mean training acc: 86.88%.
[ Wed Jan  8 00:50:10 2025 ] 	Learning Rate: 0.00032338
[ Wed Jan  8 00:50:10 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 00:50:10 2025 ] Eval epoch: 41
[ Wed Jan  8 00:50:16 2025 ] 	Mean test loss of 13 batches: 2.162859082221985.
[ Wed Jan  8 00:50:16 2025 ] 	Top1: 57.89%
[ Wed Jan  8 00:50:16 2025 ] 	Top5: 84.55%
[ Wed Jan  8 00:50:16 2025 ] Training epoch: 42
[ Wed Jan  8 00:51:55 2025 ] 	Mean training loss: 1.1447.  Mean training acc: 87.48%.
[ Wed Jan  8 00:51:55 2025 ] 	Learning Rate: 0.00031596
[ Wed Jan  8 00:51:55 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 00:51:55 2025 ] Eval epoch: 42
[ Wed Jan  8 00:52:00 2025 ] 	Mean test loss of 13 batches: 2.1748764790021458.
[ Wed Jan  8 00:52:00 2025 ] 	Top1: 57.84%
[ Wed Jan  8 00:52:00 2025 ] 	Top5: 85.02%
[ Wed Jan  8 00:52:00 2025 ] Training epoch: 43
[ Wed Jan  8 00:53:40 2025 ] 	Mean training loss: 1.1345.  Mean training acc: 87.89%.
[ Wed Jan  8 00:53:40 2025 ] 	Learning Rate: 0.00030848
[ Wed Jan  8 00:53:40 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:53:40 2025 ] Eval epoch: 43
[ Wed Jan  8 00:53:46 2025 ] 	Mean test loss of 13 batches: 2.1621450827671933.
[ Wed Jan  8 00:53:46 2025 ] 	Top1: 58.50%
[ Wed Jan  8 00:53:46 2025 ] 	Top5: 84.66%
[ Wed Jan  8 00:53:46 2025 ] Training epoch: 44
[ Wed Jan  8 00:55:25 2025 ] 	Mean training loss: 1.1279.  Mean training acc: 88.03%.
[ Wed Jan  8 00:55:25 2025 ] 	Learning Rate: 0.00030094
[ Wed Jan  8 00:55:25 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 00:55:25 2025 ] Eval epoch: 44
[ Wed Jan  8 00:55:30 2025 ] 	Mean test loss of 13 batches: 2.166119043643658.
[ Wed Jan  8 00:55:30 2025 ] 	Top1: 57.72%
[ Wed Jan  8 00:55:30 2025 ] 	Top5: 84.32%
[ Wed Jan  8 00:55:30 2025 ] Training epoch: 45
[ Wed Jan  8 00:57:10 2025 ] 	Mean training loss: 1.1181.  Mean training acc: 88.48%.
[ Wed Jan  8 00:57:10 2025 ] 	Learning Rate: 0.00029336
[ Wed Jan  8 00:57:10 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 00:57:10 2025 ] Eval epoch: 45
[ Wed Jan  8 00:57:16 2025 ] 	Mean test loss of 13 batches: 2.1653338212233324.
[ Wed Jan  8 00:57:16 2025 ] 	Top1: 58.04%
[ Wed Jan  8 00:57:16 2025 ] 	Top5: 84.35%
[ Wed Jan  8 00:57:16 2025 ] Training epoch: 46
[ Wed Jan  8 00:58:55 2025 ] 	Mean training loss: 1.1116.  Mean training acc: 88.58%.
[ Wed Jan  8 00:58:55 2025 ] 	Learning Rate: 0.00028574
[ Wed Jan  8 00:58:55 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 00:58:55 2025 ] Eval epoch: 46
[ Wed Jan  8 00:59:01 2025 ] 	Mean test loss of 13 batches: 2.1572013084705057.
[ Wed Jan  8 00:59:01 2025 ] 	Top1: 58.40%
[ Wed Jan  8 00:59:01 2025 ] 	Top5: 84.78%
[ Wed Jan  8 00:59:01 2025 ] Training epoch: 47
[ Wed Jan  8 01:00:40 2025 ] 	Mean training loss: 1.0990.  Mean training acc: 89.19%.
[ Wed Jan  8 01:00:40 2025 ] 	Learning Rate: 0.00027809
[ Wed Jan  8 01:00:40 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:00:40 2025 ] Eval epoch: 47
[ Wed Jan  8 01:00:46 2025 ] 	Mean test loss of 13 batches: 2.140835853723379.
[ Wed Jan  8 01:00:46 2025 ] 	Top1: 58.00%
[ Wed Jan  8 01:00:46 2025 ] 	Top5: 84.93%
[ Wed Jan  8 01:00:46 2025 ] Training epoch: 48
[ Wed Jan  8 01:02:25 2025 ] 	Mean training loss: 1.0948.  Mean training acc: 89.17%.
[ Wed Jan  8 01:02:25 2025 ] 	Learning Rate: 0.00027041
[ Wed Jan  8 01:02:25 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:02:25 2025 ] Eval epoch: 48
[ Wed Jan  8 01:02:30 2025 ] 	Mean test loss of 13 batches: 2.1654845751248875.
[ Wed Jan  8 01:02:30 2025 ] 	Top1: 58.45%
[ Wed Jan  8 01:02:30 2025 ] 	Top5: 84.51%
[ Wed Jan  8 01:02:30 2025 ] Training epoch: 49
[ Wed Jan  8 01:04:09 2025 ] 	Mean training loss: 1.0827.  Mean training acc: 89.68%.
[ Wed Jan  8 01:04:10 2025 ] 	Learning Rate: 0.00026273
[ Wed Jan  8 01:04:10 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:04:10 2025 ] Eval epoch: 49
[ Wed Jan  8 01:04:15 2025 ] 	Mean test loss of 13 batches: 2.1407970190048218.
[ Wed Jan  8 01:04:15 2025 ] 	Top1: 58.49%
[ Wed Jan  8 01:04:15 2025 ] 	Top5: 84.80%
[ Wed Jan  8 01:04:15 2025 ] Training epoch: 50
[ Wed Jan  8 01:05:54 2025 ] 	Mean training loss: 1.0794.  Mean training acc: 89.86%.
[ Wed Jan  8 01:05:54 2025 ] 	Learning Rate: 0.00025503
[ Wed Jan  8 01:05:54 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:05:54 2025 ] Eval epoch: 50
[ Wed Jan  8 01:06:00 2025 ] 	Mean test loss of 13 batches: 2.1483048108907847.
[ Wed Jan  8 01:06:00 2025 ] 	Top1: 58.63%
[ Wed Jan  8 01:06:00 2025 ] 	Top5: 85.18%
[ Wed Jan  8 01:06:00 2025 ] Training epoch: 51
[ Wed Jan  8 01:07:39 2025 ] 	Mean training loss: 1.0683.  Mean training acc: 90.16%.
[ Wed Jan  8 01:07:39 2025 ] 	Learning Rate: 0.00024734
[ Wed Jan  8 01:07:39 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:07:39 2025 ] Eval epoch: 51
[ Wed Jan  8 01:07:45 2025 ] 	Mean test loss of 13 batches: 2.137195898936345.
[ Wed Jan  8 01:07:45 2025 ] 	Top1: 58.41%
[ Wed Jan  8 01:07:45 2025 ] 	Top5: 85.28%
[ Wed Jan  8 01:07:45 2025 ] Training epoch: 52
[ Wed Jan  8 01:09:24 2025 ] 	Mean training loss: 1.0667.  Mean training acc: 90.20%.
[ Wed Jan  8 01:09:24 2025 ] 	Learning Rate: 0.00023965
[ Wed Jan  8 01:09:24 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:09:24 2025 ] Eval epoch: 52
[ Wed Jan  8 01:09:30 2025 ] 	Mean test loss of 13 batches: 2.1611646780600915.
[ Wed Jan  8 01:09:30 2025 ] 	Top1: 58.84%
[ Wed Jan  8 01:09:30 2025 ] 	Top5: 84.48%
[ Wed Jan  8 01:09:30 2025 ] Training epoch: 53
[ Wed Jan  8 01:11:09 2025 ] 	Mean training loss: 1.0548.  Mean training acc: 90.70%.
[ Wed Jan  8 01:11:09 2025 ] 	Learning Rate: 0.00023197
[ Wed Jan  8 01:11:09 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:11:09 2025 ] Eval epoch: 53
[ Wed Jan  8 01:11:15 2025 ] 	Mean test loss of 13 batches: 2.141954834644611.
[ Wed Jan  8 01:11:15 2025 ] 	Top1: 58.34%
[ Wed Jan  8 01:11:15 2025 ] 	Top5: 84.87%
[ Wed Jan  8 01:11:15 2025 ] Training epoch: 54
[ Wed Jan  8 01:12:54 2025 ] 	Mean training loss: 1.0467.  Mean training acc: 90.92%.
[ Wed Jan  8 01:12:54 2025 ] 	Learning Rate: 0.00022432
[ Wed Jan  8 01:12:54 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:12:54 2025 ] Eval epoch: 54
[ Wed Jan  8 01:13:00 2025 ] 	Mean test loss of 13 batches: 2.171264171600342.
[ Wed Jan  8 01:13:00 2025 ] 	Top1: 58.38%
[ Wed Jan  8 01:13:00 2025 ] 	Top5: 85.05%
[ Wed Jan  8 01:13:00 2025 ] Training epoch: 55
[ Wed Jan  8 01:14:39 2025 ] 	Mean training loss: 1.0413.  Mean training acc: 91.18%.
[ Wed Jan  8 01:14:39 2025 ] 	Learning Rate: 0.00021670
[ Wed Jan  8 01:14:39 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:14:39 2025 ] Eval epoch: 55
[ Wed Jan  8 01:14:45 2025 ] 	Mean test loss of 13 batches: 2.1372857644007754.
[ Wed Jan  8 01:14:45 2025 ] 	Top1: 58.58%
[ Wed Jan  8 01:14:45 2025 ] 	Top5: 84.98%
[ Wed Jan  8 01:14:45 2025 ] Training epoch: 56
[ Wed Jan  8 01:16:24 2025 ] 	Mean training loss: 1.0368.  Mean training acc: 91.26%.
[ Wed Jan  8 01:16:24 2025 ] 	Learning Rate: 0.00020912
[ Wed Jan  8 01:16:24 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 01:16:24 2025 ] Eval epoch: 56
[ Wed Jan  8 01:16:29 2025 ] 	Mean test loss of 13 batches: 2.1609497528809767.
[ Wed Jan  8 01:16:29 2025 ] 	Top1: 58.31%
[ Wed Jan  8 01:16:29 2025 ] 	Top5: 84.60%
[ Wed Jan  8 01:16:29 2025 ] Training epoch: 57
[ Wed Jan  8 01:18:09 2025 ] 	Mean training loss: 1.0302.  Mean training acc: 91.54%.
[ Wed Jan  8 01:18:09 2025 ] 	Learning Rate: 0.00020158
[ Wed Jan  8 01:18:09 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:18:09 2025 ] Eval epoch: 57
[ Wed Jan  8 01:18:14 2025 ] 	Mean test loss of 13 batches: 2.1447105682813206.
[ Wed Jan  8 01:18:14 2025 ] 	Top1: 58.65%
[ Wed Jan  8 01:18:14 2025 ] 	Top5: 84.68%
[ Wed Jan  8 01:18:14 2025 ] Training epoch: 58
[ Wed Jan  8 01:19:53 2025 ] 	Mean training loss: 1.0292.  Mean training acc: 91.52%.
[ Wed Jan  8 01:19:53 2025 ] 	Learning Rate: 0.00019410
[ Wed Jan  8 01:19:53 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:19:53 2025 ] Eval epoch: 58
[ Wed Jan  8 01:19:59 2025 ] 	Mean test loss of 13 batches: 2.1415932270196767.
[ Wed Jan  8 01:19:59 2025 ] 	Top1: 59.13%
[ Wed Jan  8 01:19:59 2025 ] 	Top5: 84.75%
[ Wed Jan  8 01:19:59 2025 ] Training epoch: 59
[ Wed Jan  8 01:21:38 2025 ] 	Mean training loss: 1.0158.  Mean training acc: 92.11%.
[ Wed Jan  8 01:21:38 2025 ] 	Learning Rate: 0.00018668
[ Wed Jan  8 01:21:38 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:21:39 2025 ] Eval epoch: 59
[ Wed Jan  8 01:21:44 2025 ] 	Mean test loss of 13 batches: 2.1606917014488807.
[ Wed Jan  8 01:21:44 2025 ] 	Top1: 58.52%
[ Wed Jan  8 01:21:44 2025 ] 	Top5: 84.37%
[ Wed Jan  8 01:21:44 2025 ] Training epoch: 60
[ Wed Jan  8 01:23:23 2025 ] 	Mean training loss: 1.0132.  Mean training acc: 92.12%.
[ Wed Jan  8 01:23:23 2025 ] 	Learning Rate: 0.00017932
[ Wed Jan  8 01:23:23 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:23:23 2025 ] Eval epoch: 60
[ Wed Jan  8 01:23:29 2025 ] 	Mean test loss of 13 batches: 2.154480292246892.
[ Wed Jan  8 01:23:29 2025 ] 	Top1: 58.31%
[ Wed Jan  8 01:23:29 2025 ] 	Top5: 84.59%
[ Wed Jan  8 01:23:29 2025 ] Training epoch: 61
[ Wed Jan  8 01:25:08 2025 ] 	Mean training loss: 1.0072.  Mean training acc: 92.31%.
[ Wed Jan  8 01:25:08 2025 ] 	Learning Rate: 0.00017204
[ Wed Jan  8 01:25:08 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 01:25:08 2025 ] Eval epoch: 61
[ Wed Jan  8 01:25:13 2025 ] 	Mean test loss of 13 batches: 2.1684809923171997.
[ Wed Jan  8 01:25:13 2025 ] 	Top1: 58.34%
[ Wed Jan  8 01:25:13 2025 ] 	Top5: 84.39%
[ Wed Jan  8 01:25:13 2025 ] Training epoch: 62
[ Wed Jan  8 01:26:53 2025 ] 	Mean training loss: 1.0020.  Mean training acc: 92.56%.
[ Wed Jan  8 01:26:53 2025 ] 	Learning Rate: 0.00016484
[ Wed Jan  8 01:26:53 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:26:53 2025 ] Eval epoch: 62
[ Wed Jan  8 01:26:59 2025 ] 	Mean test loss of 13 batches: 2.134398946395287.
[ Wed Jan  8 01:26:59 2025 ] 	Top1: 58.81%
[ Wed Jan  8 01:26:59 2025 ] 	Top5: 84.68%
[ Wed Jan  8 01:26:59 2025 ] Training epoch: 63
[ Wed Jan  8 01:28:38 2025 ] 	Mean training loss: 0.9982.  Mean training acc: 92.56%.
[ Wed Jan  8 01:28:38 2025 ] 	Learning Rate: 0.00015773
[ Wed Jan  8 01:28:38 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:28:38 2025 ] Eval epoch: 63
[ Wed Jan  8 01:28:44 2025 ] 	Mean test loss of 13 batches: 2.1420489091139574.
[ Wed Jan  8 01:28:44 2025 ] 	Top1: 58.54%
[ Wed Jan  8 01:28:44 2025 ] 	Top5: 84.43%
[ Wed Jan  8 01:28:44 2025 ] Training epoch: 64
[ Wed Jan  8 01:30:23 2025 ] 	Mean training loss: 0.9920.  Mean training acc: 92.84%.
[ Wed Jan  8 01:30:23 2025 ] 	Learning Rate: 0.00015071
[ Wed Jan  8 01:30:23 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:30:23 2025 ] Eval epoch: 64
[ Wed Jan  8 01:30:28 2025 ] 	Mean test loss of 13 batches: 2.155120171033419.
[ Wed Jan  8 01:30:28 2025 ] 	Top1: 58.52%
[ Wed Jan  8 01:30:28 2025 ] 	Top5: 84.64%
[ Wed Jan  8 01:30:28 2025 ] Training epoch: 65
[ Wed Jan  8 01:32:08 2025 ] 	Mean training loss: 0.9878.  Mean training acc: 92.99%.
[ Wed Jan  8 01:32:08 2025 ] 	Learning Rate: 0.00014380
[ Wed Jan  8 01:32:08 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:32:08 2025 ] Eval epoch: 65
[ Wed Jan  8 01:32:14 2025 ] 	Mean test loss of 13 batches: 2.1477985932276797.
[ Wed Jan  8 01:32:14 2025 ] 	Top1: 58.70%
[ Wed Jan  8 01:32:14 2025 ] 	Top5: 84.46%
[ Wed Jan  8 01:32:14 2025 ] Training epoch: 66
[ Wed Jan  8 01:33:53 2025 ] 	Mean training loss: 0.9869.  Mean training acc: 92.96%.
[ Wed Jan  8 01:33:53 2025 ] 	Learning Rate: 0.00013700
[ Wed Jan  8 01:33:53 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:33:53 2025 ] Eval epoch: 66
[ Wed Jan  8 01:33:59 2025 ] 	Mean test loss of 13 batches: 2.1498621702194214.
[ Wed Jan  8 01:33:59 2025 ] 	Top1: 58.38%
[ Wed Jan  8 01:33:59 2025 ] 	Top5: 84.77%
[ Wed Jan  8 01:33:59 2025 ] Training epoch: 67
[ Wed Jan  8 01:35:38 2025 ] 	Mean training loss: 0.9817.  Mean training acc: 93.18%.
[ Wed Jan  8 01:35:38 2025 ] 	Learning Rate: 0.00013031
[ Wed Jan  8 01:35:38 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:35:38 2025 ] Eval epoch: 67
[ Wed Jan  8 01:35:44 2025 ] 	Mean test loss of 13 batches: 2.1538529120958767.
[ Wed Jan  8 01:35:44 2025 ] 	Top1: 59.00%
[ Wed Jan  8 01:35:44 2025 ] 	Top5: 84.09%
[ Wed Jan  8 01:35:44 2025 ] Training epoch: 68
[ Wed Jan  8 01:37:25 2025 ] 	Mean training loss: 0.9747.  Mean training acc: 93.49%.
[ Wed Jan  8 01:37:25 2025 ] 	Learning Rate: 0.00012375
[ Wed Jan  8 01:37:25 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:37:25 2025 ] Eval epoch: 68
[ Wed Jan  8 01:37:30 2025 ] 	Mean test loss of 13 batches: 2.1470200281876783.
[ Wed Jan  8 01:37:30 2025 ] 	Top1: 58.61%
[ Wed Jan  8 01:37:30 2025 ] 	Top5: 84.78%
[ Wed Jan  8 01:37:30 2025 ] Training epoch: 69
[ Wed Jan  8 01:39:11 2025 ] 	Mean training loss: 0.9719.  Mean training acc: 93.62%.
[ Wed Jan  8 01:39:11 2025 ] 	Learning Rate: 0.00011731
[ Wed Jan  8 01:39:11 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:39:11 2025 ] Eval epoch: 69
[ Wed Jan  8 01:39:17 2025 ] 	Mean test loss of 13 batches: 2.1363235161854672.
[ Wed Jan  8 01:39:17 2025 ] 	Top1: 59.02%
[ Wed Jan  8 01:39:17 2025 ] 	Top5: 84.78%
[ Wed Jan  8 01:39:17 2025 ] Training epoch: 70
[ Wed Jan  8 01:40:56 2025 ] 	Mean training loss: 0.9687.  Mean training acc: 93.59%.
[ Wed Jan  8 01:40:56 2025 ] 	Learning Rate: 0.00011102
[ Wed Jan  8 01:40:56 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 01:40:56 2025 ] Eval epoch: 70
[ Wed Jan  8 01:41:02 2025 ] 	Mean test loss of 13 batches: 2.149756220670847.
[ Wed Jan  8 01:41:02 2025 ] 	Top1: 58.68%
[ Wed Jan  8 01:41:02 2025 ] 	Top5: 84.35%
[ Wed Jan  8 01:41:02 2025 ] Training epoch: 71
[ Wed Jan  8 01:42:42 2025 ] 	Mean training loss: 0.9626.  Mean training acc: 93.89%.
[ Wed Jan  8 01:42:42 2025 ] 	Learning Rate: 0.00010486
[ Wed Jan  8 01:42:42 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:42:43 2025 ] Eval epoch: 71
[ Wed Jan  8 01:42:48 2025 ] 	Mean test loss of 13 batches: 2.1488176675943227.
[ Wed Jan  8 01:42:48 2025 ] 	Top1: 58.40%
[ Wed Jan  8 01:42:48 2025 ] 	Top5: 84.43%
[ Wed Jan  8 01:42:48 2025 ] Training epoch: 72
[ Wed Jan  8 01:44:27 2025 ] 	Mean training loss: 0.9571.  Mean training acc: 94.02%.
[ Wed Jan  8 01:44:27 2025 ] 	Learning Rate: 0.00009885
[ Wed Jan  8 01:44:27 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 01:44:27 2025 ] Eval epoch: 72
[ Wed Jan  8 01:44:33 2025 ] 	Mean test loss of 13 batches: 2.1340647018872776.
[ Wed Jan  8 01:44:33 2025 ] 	Top1: 58.99%
[ Wed Jan  8 01:44:33 2025 ] 	Top5: 84.59%
[ Wed Jan  8 01:44:33 2025 ] Training epoch: 73
[ Wed Jan  8 01:46:12 2025 ] 	Mean training loss: 0.9580.  Mean training acc: 94.04%.
[ Wed Jan  8 01:46:12 2025 ] 	Learning Rate: 0.00009300
[ Wed Jan  8 01:46:12 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:46:12 2025 ] Eval epoch: 73
[ Wed Jan  8 01:46:18 2025 ] 	Mean test loss of 13 batches: 2.1398546420610867.
[ Wed Jan  8 01:46:18 2025 ] 	Top1: 59.09%
[ Wed Jan  8 01:46:18 2025 ] 	Top5: 84.30%
[ Wed Jan  8 01:46:18 2025 ] Training epoch: 74
[ Wed Jan  8 01:47:57 2025 ] 	Mean training loss: 0.9535.  Mean training acc: 94.13%.
[ Wed Jan  8 01:47:57 2025 ] 	Learning Rate: 0.00008731
[ Wed Jan  8 01:47:57 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:47:57 2025 ] Eval epoch: 74
[ Wed Jan  8 01:48:02 2025 ] 	Mean test loss of 13 batches: 2.138863178399893.
[ Wed Jan  8 01:48:02 2025 ] 	Top1: 58.34%
[ Wed Jan  8 01:48:02 2025 ] 	Top5: 84.50%
[ Wed Jan  8 01:48:02 2025 ] Training epoch: 75
[ Wed Jan  8 01:49:42 2025 ] 	Mean training loss: 0.9483.  Mean training acc: 94.40%.
[ Wed Jan  8 01:49:42 2025 ] 	Learning Rate: 0.00008178
[ Wed Jan  8 01:49:42 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:49:42 2025 ] Eval epoch: 75
[ Wed Jan  8 01:49:47 2025 ] 	Mean test loss of 13 batches: 2.1472049126258264.
[ Wed Jan  8 01:49:47 2025 ] 	Top1: 58.49%
[ Wed Jan  8 01:49:47 2025 ] 	Top5: 84.46%
[ Wed Jan  8 01:49:47 2025 ] Training epoch: 76
[ Wed Jan  8 01:51:26 2025 ] 	Mean training loss: 0.9476.  Mean training acc: 94.37%.
[ Wed Jan  8 01:51:26 2025 ] 	Learning Rate: 0.00007642
[ Wed Jan  8 01:51:26 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:51:26 2025 ] Eval epoch: 76
[ Wed Jan  8 01:51:31 2025 ] 	Mean test loss of 13 batches: 2.13630473613739.
[ Wed Jan  8 01:51:32 2025 ] 	Top1: 59.09%
[ Wed Jan  8 01:51:32 2025 ] 	Top5: 84.68%
[ Wed Jan  8 01:51:32 2025 ] Training epoch: 77
[ Wed Jan  8 01:53:10 2025 ] 	Mean training loss: 0.9425.  Mean training acc: 94.55%.
[ Wed Jan  8 01:53:10 2025 ] 	Learning Rate: 0.00007124
[ Wed Jan  8 01:53:10 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 01:53:10 2025 ] Eval epoch: 77
[ Wed Jan  8 01:53:15 2025 ] 	Mean test loss of 13 batches: 2.1367449118540836.
[ Wed Jan  8 01:53:15 2025 ] 	Top1: 58.97%
[ Wed Jan  8 01:53:15 2025 ] 	Top5: 84.60%
[ Wed Jan  8 01:53:15 2025 ] Training epoch: 78
[ Wed Jan  8 01:54:53 2025 ] 	Mean training loss: 0.9395.  Mean training acc: 94.65%.
[ Wed Jan  8 01:54:53 2025 ] 	Learning Rate: 0.00006624
[ Wed Jan  8 01:54:53 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 01:54:53 2025 ] Eval epoch: 78
[ Wed Jan  8 01:54:58 2025 ] 	Mean test loss of 13 batches: 2.137512527979337.
[ Wed Jan  8 01:54:58 2025 ] 	Top1: 58.75%
[ Wed Jan  8 01:54:58 2025 ] 	Top5: 84.55%
[ Wed Jan  8 01:54:58 2025 ] Training epoch: 79
[ Wed Jan  8 01:56:39 2025 ] 	Mean training loss: 0.9385.  Mean training acc: 94.70%.
[ Wed Jan  8 01:56:39 2025 ] 	Learning Rate: 0.00006143
[ Wed Jan  8 01:56:39 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:56:39 2025 ] Eval epoch: 79
[ Wed Jan  8 01:56:44 2025 ] 	Mean test loss of 13 batches: 2.1419650316238403.
[ Wed Jan  8 01:56:44 2025 ] 	Top1: 59.00%
[ Wed Jan  8 01:56:44 2025 ] 	Top5: 84.32%
[ Wed Jan  8 01:56:44 2025 ] Training epoch: 80
[ Wed Jan  8 01:58:24 2025 ] 	Mean training loss: 0.9357.  Mean training acc: 94.87%.
[ Wed Jan  8 01:58:24 2025 ] 	Learning Rate: 0.00005681
[ Wed Jan  8 01:58:24 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 01:58:25 2025 ] Eval epoch: 80
[ Wed Jan  8 01:58:30 2025 ] 	Mean test loss of 13 batches: 2.142175032542302.
[ Wed Jan  8 01:58:30 2025 ] 	Top1: 58.72%
[ Wed Jan  8 01:58:30 2025 ] 	Top5: 84.53%
[ Wed Jan  8 01:58:30 2025 ] Training epoch: 81
[ Wed Jan  8 02:00:10 2025 ] 	Mean training loss: 0.9314.  Mean training acc: 94.82%.
[ Wed Jan  8 02:00:10 2025 ] 	Learning Rate: 0.00005238
[ Wed Jan  8 02:00:10 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 02:00:10 2025 ] Eval epoch: 81
[ Wed Jan  8 02:00:15 2025 ] 	Mean test loss of 13 batches: 2.1395459725306583.
[ Wed Jan  8 02:00:15 2025 ] 	Top1: 58.88%
[ Wed Jan  8 02:00:15 2025 ] 	Top5: 84.30%
[ Wed Jan  8 02:00:15 2025 ] Training epoch: 82
[ Wed Jan  8 02:01:54 2025 ] 	Mean training loss: 0.9307.  Mean training acc: 94.97%.
[ Wed Jan  8 02:01:54 2025 ] 	Learning Rate: 0.00004816
[ Wed Jan  8 02:01:54 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 02:01:54 2025 ] Eval epoch: 82
[ Wed Jan  8 02:01:59 2025 ] 	Mean test loss of 13 batches: 2.1382469030526967.
[ Wed Jan  8 02:01:59 2025 ] 	Top1: 58.83%
[ Wed Jan  8 02:01:59 2025 ] 	Top5: 84.30%
[ Wed Jan  8 02:01:59 2025 ] Training epoch: 83
[ Wed Jan  8 02:03:37 2025 ] 	Mean training loss: 0.9259.  Mean training acc: 95.16%.
[ Wed Jan  8 02:03:37 2025 ] 	Learning Rate: 0.00004413
[ Wed Jan  8 02:03:37 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:03:37 2025 ] Eval epoch: 83
[ Wed Jan  8 02:03:42 2025 ] 	Mean test loss of 13 batches: 2.1413048597482534.
[ Wed Jan  8 02:03:42 2025 ] 	Top1: 58.84%
[ Wed Jan  8 02:03:42 2025 ] 	Top5: 84.28%
[ Wed Jan  8 02:03:42 2025 ] Training epoch: 84
[ Wed Jan  8 02:05:21 2025 ] 	Mean training loss: 0.9253.  Mean training acc: 95.18%.
[ Wed Jan  8 02:05:21 2025 ] 	Learning Rate: 0.00004032
[ Wed Jan  8 02:05:21 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:05:21 2025 ] Eval epoch: 84
[ Wed Jan  8 02:05:26 2025 ] 	Mean test loss of 13 batches: 2.1355122786301832.
[ Wed Jan  8 02:05:26 2025 ] 	Top1: 58.81%
[ Wed Jan  8 02:05:26 2025 ] 	Top5: 84.55%
[ Wed Jan  8 02:05:26 2025 ] Training epoch: 85
[ Wed Jan  8 02:07:06 2025 ] 	Mean training loss: 0.9239.  Mean training acc: 95.14%.
[ Wed Jan  8 02:07:06 2025 ] 	Learning Rate: 0.00003672
[ Wed Jan  8 02:07:06 2025 ] 	Time consumption: [Data]05%, [Network]94%
[ Wed Jan  8 02:07:06 2025 ] Eval epoch: 85
[ Wed Jan  8 02:07:12 2025 ] 	Mean test loss of 13 batches: 2.137803160227262.
[ Wed Jan  8 02:07:12 2025 ] 	Top1: 58.92%
[ Wed Jan  8 02:07:12 2025 ] 	Top5: 84.23%
[ Wed Jan  8 02:07:12 2025 ] Training epoch: 86
[ Wed Jan  8 02:08:50 2025 ] 	Mean training loss: 0.9219.  Mean training acc: 95.26%.
[ Wed Jan  8 02:08:50 2025 ] 	Learning Rate: 0.00003333
[ Wed Jan  8 02:08:50 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 02:08:50 2025 ] Eval epoch: 86
[ Wed Jan  8 02:08:56 2025 ] 	Mean test loss of 13 batches: 2.1333088324620175.
[ Wed Jan  8 02:08:56 2025 ] 	Top1: 58.84%
[ Wed Jan  8 02:08:56 2025 ] 	Top5: 84.71%
[ Wed Jan  8 02:08:56 2025 ] Training epoch: 87
[ Wed Jan  8 02:10:35 2025 ] 	Mean training loss: 0.9207.  Mean training acc: 95.36%.
[ Wed Jan  8 02:10:35 2025 ] 	Learning Rate: 0.00003016
[ Wed Jan  8 02:10:35 2025 ] 	Time consumption: [Data]06%, [Network]94%
[ Wed Jan  8 02:10:35 2025 ] Eval epoch: 87
[ Wed Jan  8 02:10:40 2025 ] 	Mean test loss of 13 batches: 2.1302093175741343.
[ Wed Jan  8 02:10:40 2025 ] 	Top1: 58.72%
[ Wed Jan  8 02:10:40 2025 ] 	Top5: 84.51%
[ Wed Jan  8 02:10:40 2025 ] Training epoch: 88
[ Wed Jan  8 02:12:18 2025 ] 	Mean training loss: 0.9202.  Mean training acc: 95.32%.
[ Wed Jan  8 02:12:18 2025 ] 	Learning Rate: 0.00002722
[ Wed Jan  8 02:12:18 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:12:18 2025 ] Eval epoch: 88
[ Wed Jan  8 02:12:23 2025 ] 	Mean test loss of 13 batches: 2.1362535770122824.
[ Wed Jan  8 02:12:23 2025 ] 	Top1: 58.74%
[ Wed Jan  8 02:12:23 2025 ] 	Top5: 84.48%
[ Wed Jan  8 02:12:23 2025 ] Training epoch: 89
[ Wed Jan  8 02:14:01 2025 ] 	Mean training loss: 0.9177.  Mean training acc: 95.38%.
[ Wed Jan  8 02:14:01 2025 ] 	Learning Rate: 0.00002449
[ Wed Jan  8 02:14:01 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:14:02 2025 ] Eval epoch: 89
[ Wed Jan  8 02:14:07 2025 ] 	Mean test loss of 13 batches: 2.1349242467146654.
[ Wed Jan  8 02:14:07 2025 ] 	Top1: 58.95%
[ Wed Jan  8 02:14:07 2025 ] 	Top5: 84.57%
[ Wed Jan  8 02:14:07 2025 ] Training epoch: 90
[ Wed Jan  8 02:15:45 2025 ] 	Mean training loss: 0.9182.  Mean training acc: 95.45%.
[ Wed Jan  8 02:15:45 2025 ] 	Learning Rate: 0.00002200
[ Wed Jan  8 02:15:45 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:15:45 2025 ] Eval epoch: 90
[ Wed Jan  8 02:15:50 2025 ] 	Mean test loss of 13 batches: 2.13835098193242.
[ Wed Jan  8 02:15:50 2025 ] 	Top1: 59.09%
[ Wed Jan  8 02:15:50 2025 ] 	Top5: 84.59%
[ Wed Jan  8 02:15:50 2025 ] Training epoch: 91
[ Wed Jan  8 02:17:28 2025 ] 	Mean training loss: 0.9205.  Mean training acc: 95.30%.
[ Wed Jan  8 02:17:28 2025 ] 	Learning Rate: 0.00001974
[ Wed Jan  8 02:17:28 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:17:28 2025 ] Eval epoch: 91
[ Wed Jan  8 02:17:34 2025 ] 	Mean test loss of 13 batches: 2.1394040309465847.
[ Wed Jan  8 02:17:34 2025 ] 	Top1: 58.84%
[ Wed Jan  8 02:17:34 2025 ] 	Top5: 84.37%
[ Wed Jan  8 02:17:34 2025 ] Training epoch: 92
[ Wed Jan  8 02:19:11 2025 ] 	Mean training loss: 0.9129.  Mean training acc: 95.56%.
[ Wed Jan  8 02:19:11 2025 ] 	Learning Rate: 0.00001770
[ Wed Jan  8 02:19:11 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:19:11 2025 ] Eval epoch: 92
[ Wed Jan  8 02:19:17 2025 ] 	Mean test loss of 13 batches: 2.136617798071641.
[ Wed Jan  8 02:19:17 2025 ] 	Top1: 58.81%
[ Wed Jan  8 02:19:17 2025 ] 	Top5: 84.59%
[ Wed Jan  8 02:19:17 2025 ] Training epoch: 93
[ Wed Jan  8 02:20:54 2025 ] 	Mean training loss: 0.9176.  Mean training acc: 95.34%.
[ Wed Jan  8 02:20:54 2025 ] 	Learning Rate: 0.00001591
[ Wed Jan  8 02:20:54 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:20:54 2025 ] Eval epoch: 93
[ Wed Jan  8 02:21:00 2025 ] 	Mean test loss of 13 batches: 2.135947823524475.
[ Wed Jan  8 02:21:00 2025 ] 	Top1: 58.92%
[ Wed Jan  8 02:21:00 2025 ] 	Top5: 84.50%
[ Wed Jan  8 02:21:00 2025 ] Training epoch: 94
[ Wed Jan  8 02:22:37 2025 ] 	Mean training loss: 0.9142.  Mean training acc: 95.48%.
[ Wed Jan  8 02:22:37 2025 ] 	Learning Rate: 0.00001435
[ Wed Jan  8 02:22:37 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:22:38 2025 ] Eval epoch: 94
[ Wed Jan  8 02:22:43 2025 ] 	Mean test loss of 13 batches: 2.1408211451310377.
[ Wed Jan  8 02:22:43 2025 ] 	Top1: 58.52%
[ Wed Jan  8 02:22:43 2025 ] 	Top5: 84.44%
[ Wed Jan  8 02:22:43 2025 ] Training epoch: 95
[ Wed Jan  8 02:24:21 2025 ] 	Mean training loss: 0.9143.  Mean training acc: 95.56%.
[ Wed Jan  8 02:24:21 2025 ] 	Learning Rate: 0.00001302
[ Wed Jan  8 02:24:21 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:24:21 2025 ] Eval epoch: 95
[ Wed Jan  8 02:24:26 2025 ] 	Mean test loss of 13 batches: 2.1395862194207997.
[ Wed Jan  8 02:24:26 2025 ] 	Top1: 58.74%
[ Wed Jan  8 02:24:26 2025 ] 	Top5: 84.43%
[ Wed Jan  8 02:24:26 2025 ] Training epoch: 96
[ Wed Jan  8 02:26:04 2025 ] 	Mean training loss: 0.9137.  Mean training acc: 95.52%.
[ Wed Jan  8 02:26:04 2025 ] 	Learning Rate: 0.00001194
[ Wed Jan  8 02:26:04 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:26:04 2025 ] Eval epoch: 96
[ Wed Jan  8 02:26:09 2025 ] 	Mean test loss of 13 batches: 2.1364293648646426.
[ Wed Jan  8 02:26:09 2025 ] 	Top1: 58.68%
[ Wed Jan  8 02:26:09 2025 ] 	Top5: 84.50%
[ Wed Jan  8 02:26:09 2025 ] Training epoch: 97
[ Wed Jan  8 02:27:47 2025 ] 	Mean training loss: 0.9093.  Mean training acc: 95.70%.
[ Wed Jan  8 02:27:47 2025 ] 	Learning Rate: 0.00001109
[ Wed Jan  8 02:27:47 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:27:47 2025 ] Eval epoch: 97
[ Wed Jan  8 02:27:53 2025 ] 	Mean test loss of 13 batches: 2.1366638311972985.
[ Wed Jan  8 02:27:53 2025 ] 	Top1: 58.86%
[ Wed Jan  8 02:27:53 2025 ] 	Top5: 84.69%
[ Wed Jan  8 02:27:53 2025 ] Training epoch: 98
[ Wed Jan  8 02:29:31 2025 ] 	Mean training loss: 0.9118.  Mean training acc: 95.57%.
[ Wed Jan  8 02:29:31 2025 ] 	Learning Rate: 0.00001049
[ Wed Jan  8 02:29:31 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:29:31 2025 ] Eval epoch: 98
[ Wed Jan  8 02:29:36 2025 ] 	Mean test loss of 13 batches: 2.140562424292931.
[ Wed Jan  8 02:29:36 2025 ] 	Top1: 58.75%
[ Wed Jan  8 02:29:36 2025 ] 	Top5: 84.43%
[ Wed Jan  8 02:29:36 2025 ] Training epoch: 99
[ Wed Jan  8 02:31:14 2025 ] 	Mean training loss: 0.9116.  Mean training acc: 95.60%.
[ Wed Jan  8 02:31:14 2025 ] 	Learning Rate: 0.00001012
[ Wed Jan  8 02:31:14 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:31:14 2025 ] Eval epoch: 99
[ Wed Jan  8 02:31:19 2025 ] 	Mean test loss of 13 batches: 2.1391321695767918.
[ Wed Jan  8 02:31:19 2025 ] 	Top1: 58.93%
[ Wed Jan  8 02:31:19 2025 ] 	Top5: 84.50%
[ Wed Jan  8 02:31:19 2025 ] Training epoch: 100
[ Wed Jan  8 02:32:57 2025 ] 	Mean training loss: 0.9099.  Mean training acc: 95.68%.
[ Wed Jan  8 02:32:57 2025 ] 	Learning Rate: 0.00001000
[ Wed Jan  8 02:32:57 2025 ] 	Time consumption: [Data]05%, [Network]95%
[ Wed Jan  8 02:32:57 2025 ] Eval epoch: 100
[ Wed Jan  8 02:33:02 2025 ] 	Mean test loss of 13 batches: 2.138327479362488.
[ Wed Jan  8 02:33:02 2025 ] 	Top1: 58.52%
[ Wed Jan  8 02:33:02 2025 ] 	Top5: 84.64%
[ Wed Jan  8 02:33:07 2025 ] Best accuracy: 0.5912996777658431
[ Wed Jan  8 02:33:07 2025 ] Epoch number: 58
[ Wed Jan  8 02:33:07 2025 ] Model name: ./output/original_48_s/
[ Wed Jan  8 02:33:07 2025 ] Model total number of params: 2151804
[ Wed Jan  8 02:33:07 2025 ] Weight decay: 0.01
[ Wed Jan  8 02:33:07 2025 ] Base LR: 0.0005
[ Wed Jan  8 02:33:07 2025 ] Batch Size: 448
[ Wed Jan  8 02:33:07 2025 ] Test Batch Size: 448
[ Wed Jan  8 02:33:07 2025 ] seed: 1
[ Thu Feb 13 20:14:36 2025 ] Load weights from ./output/original_48/runs-84-73752.pt.
[ Thu Feb 13 20:15:14 2025 ] Load weights from output/original_48_s/runs-58-14558.pt.
[ Thu Feb 13 20:15:20 2025 ] using warm up, epoch: 10
[ Thu Feb 13 20:16:09 2025 ] Load weights from output/original_48_s/runs-58-14558.pt.
[ Thu Feb 13 20:16:14 2025 ] using warm up, epoch: 10
[ Thu Feb 13 20:18:03 2025 ] Load weights from output/original_48_s/runs-58-14558.pt.
[ Thu Feb 13 20:18:09 2025 ] using warm up, epoch: 10
[ Thu Feb 13 20:18:09 2025 ] Parameters:
{'work_dir': './output/original_48_s/', 'model_saved_name': './output/original_48_s/runs', 'config': './config/SkateFormer_j_s_NEW.yaml', 'weights': 'output/original_48_s/runs-58-14558.pt', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ma52_NEW.Feeder', 'num_worker': 6, 'train_feeder_args': {'data_path': 'new_ma52/json', 'label_path': 'train', 'data_type': 'j', 'repeat': 10, 'p': 0.3, 'debug': False, 'partition': True}, 'test_feeder_args': {'data_path': 'new_ma52/json', 'label_path': 'val', 'data_type': 'j', 'repeat': 1, 'partition': True}, 'model': 'model.SkateFormer.SkateFormer_', 'model_args': {'in_channels': 2, 'num_classes': 52, 'num_people': 1, 'num_points': 48, 'kernel_size': 7, 'num_heads': 16, 'attn_drop': 0.5, 'head_drop': 0.0, 'rel': True, 'drop_path': 0.3, 'type_1_size': [8, 12], 'type_2_size': [8, 24], 'type_3_size': [8, 12], 'type_4_size': [8, 24], 'mlp_ratio': 0.8, 'index_t': True}, 'ignore_weights': [], 'base_lr': 0.0005, 'min_lr': 1e-05, 'warmup_lr': 1e-07, 'warmup_prefix': False, 'warm_up_epoch': 10, 'grad_clip': True, 'grad_max': 1.0, 'device': [0], 'optimizer': 'AdamW', 'lr_scheduler': 'cosine', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 100, 'weight_decay': 0.01, 'lr_ratio': 0.001, 'lr_decay_rate': 0.1, 'loss_type': 'LSCE'}

[ Thu Feb 13 20:18:09 2025 ] # Parameters: 2151804
[ Thu Feb 13 20:18:09 2025 ] Eval epoch: 1
[ Thu Feb 13 20:20:35 2025 ] Load weights from output/original_48_s/runs-58-14558.pt.
[ Thu Feb 13 20:21:01 2025 ] Load weights from output/original_48_s/runs-58-14558.pt.
[ Thu Feb 13 20:21:07 2025 ] using warm up, epoch: 10
[ Thu Feb 13 20:21:07 2025 ] Parameters:
{'work_dir': './output/original_48_s/', 'model_saved_name': './output/original_48_s/runs', 'config': './config/SkateFormer_j_s_NEW.yaml', 'weights': 'output/original_48_s/runs-58-14558.pt', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 1, 'save_epoch': 30, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'feeders.feeder_ma52_NEW.Feeder', 'num_worker': 6, 'train_feeder_args': {'data_path': 'new_ma52/json', 'label_path': 'train', 'data_type': 'j', 'repeat': 10, 'p': 0.3, 'debug': False, 'partition': True}, 'test_feeder_args': {'data_path': 'new_ma52/json', 'label_path': 'val', 'data_type': 'j', 'repeat': 1, 'partition': True}, 'model': 'model.SkateFormer.SkateFormer_', 'model_args': {'in_channels': 2, 'num_classes': 52, 'num_people': 1, 'num_points': 48, 'kernel_size': 7, 'num_heads': 16, 'attn_drop': 0.5, 'head_drop': 0.0, 'rel': True, 'drop_path': 0.3, 'type_1_size': [8, 12], 'type_2_size': [8, 24], 'type_3_size': [8, 12], 'type_4_size': [8, 24], 'mlp_ratio': 0.8, 'index_t': True}, 'ignore_weights': [], 'base_lr': 0.0005, 'min_lr': 1e-05, 'warmup_lr': 1e-07, 'warmup_prefix': False, 'warm_up_epoch': 10, 'grad_clip': True, 'grad_max': 1.0, 'device': [0], 'optimizer': 'AdamW', 'lr_scheduler': 'cosine', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 100, 'weight_decay': 0.01, 'lr_ratio': 0.001, 'lr_decay_rate': 0.1, 'loss_type': 'LSCE'}

[ Thu Feb 13 20:21:07 2025 ] # Parameters: 2151804
[ Thu Feb 13 20:21:07 2025 ] Eval epoch: 1
