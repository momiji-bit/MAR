{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-22T16:16:03.847525Z",
     "start_time": "2025-02-22T16:16:03.829585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 加载.pt文件\n",
    "pt_file = \"output/original_contrast2/runs-1-703.pt\"  # 请替换成你的文件路径\n",
    "model = torch.load(pt_file, map_location=torch.device('cpu'))  # 读取文件到CPU\n",
    "\n",
    "# 如果是 state_dict，直接打印各层的名称和维度\n",
    "if isinstance(model, dict):\n",
    "    state_dict = model.get('state_dict', model)  # 有些模型可能嵌套在 'state_dict' 字典内\n",
    "    print(\"Layers and their shapes:\")\n",
    "    for name, param in state_dict.items():\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "else:\n",
    "    print(\"Loaded object is not a state_dict. It might be a complete model.\")\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers and their shapes:\n",
      "joint_person_embedding: torch.Size([96, 48])\n",
      "stem.0.weight: torch.Size([4, 2, 1, 1])\n",
      "stem.0.bias: torch.Size([4])\n",
      "stem.2.weight: torch.Size([6, 4, 1, 1])\n",
      "stem.2.bias: torch.Size([6])\n",
      "stem.4.weight: torch.Size([96, 6, 1, 1])\n",
      "stem.4.bias: torch.Size([96])\n",
      "stages.0.blocks.0.transformer.gconv: torch.Size([8, 48, 48])\n",
      "stages.0.blocks.0.transformer.norm_1.weight: torch.Size([96])\n",
      "stages.0.blocks.0.transformer.norm_1.bias: torch.Size([96])\n",
      "stages.0.blocks.0.transformer.mapping.weight: torch.Size([192, 96])\n",
      "stages.0.blocks.0.transformer.mapping.bias: torch.Size([192])\n",
      "stages.0.blocks.0.transformer.tconv.weight: torch.Size([24, 3, 7, 1])\n",
      "stages.0.blocks.0.transformer.tconv.bias: torch.Size([24])\n",
      "stages.0.blocks.0.transformer.attention.0.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.0.blocks.0.transformer.attention.0.relative_position_index: torch.Size([8, 8])\n",
      "stages.0.blocks.0.transformer.attention.1.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.0.blocks.0.transformer.attention.1.relative_position_index: torch.Size([8, 8])\n",
      "stages.0.blocks.0.transformer.attention.2.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.0.blocks.0.transformer.attention.2.relative_position_index: torch.Size([8, 8])\n",
      "stages.0.blocks.0.transformer.attention.3.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.0.blocks.0.transformer.attention.3.relative_position_index: torch.Size([8, 8])\n",
      "stages.0.blocks.0.transformer.proj.weight: torch.Size([96, 96])\n",
      "stages.0.blocks.0.transformer.proj.bias: torch.Size([96])\n",
      "stages.0.blocks.0.transformer.norm_2.weight: torch.Size([96])\n",
      "stages.0.blocks.0.transformer.norm_2.bias: torch.Size([96])\n",
      "stages.0.blocks.0.transformer.mlp.fc1.weight: torch.Size([96, 96])\n",
      "stages.0.blocks.0.transformer.mlp.fc1.bias: torch.Size([96])\n",
      "stages.0.blocks.0.transformer.mlp.fc2.weight: torch.Size([96, 96])\n",
      "stages.0.blocks.0.transformer.mlp.fc2.bias: torch.Size([96])\n",
      "stages.0.blocks.1.transformer.gconv: torch.Size([8, 48, 48])\n",
      "stages.0.blocks.1.transformer.norm_1.weight: torch.Size([96])\n",
      "stages.0.blocks.1.transformer.norm_1.bias: torch.Size([96])\n",
      "stages.0.blocks.1.transformer.mapping.weight: torch.Size([192, 96])\n",
      "stages.0.blocks.1.transformer.mapping.bias: torch.Size([192])\n",
      "stages.0.blocks.1.transformer.tconv.weight: torch.Size([24, 3, 7, 1])\n",
      "stages.0.blocks.1.transformer.tconv.bias: torch.Size([24])\n",
      "stages.0.blocks.1.transformer.attention.0.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.0.blocks.1.transformer.attention.0.relative_position_index: torch.Size([8, 8])\n",
      "stages.0.blocks.1.transformer.attention.1.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.0.blocks.1.transformer.attention.1.relative_position_index: torch.Size([8, 8])\n",
      "stages.0.blocks.1.transformer.attention.2.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.0.blocks.1.transformer.attention.2.relative_position_index: torch.Size([8, 8])\n",
      "stages.0.blocks.1.transformer.attention.3.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.0.blocks.1.transformer.attention.3.relative_position_index: torch.Size([8, 8])\n",
      "stages.0.blocks.1.transformer.proj.weight: torch.Size([96, 96])\n",
      "stages.0.blocks.1.transformer.proj.bias: torch.Size([96])\n",
      "stages.0.blocks.1.transformer.norm_2.weight: torch.Size([96])\n",
      "stages.0.blocks.1.transformer.norm_2.bias: torch.Size([96])\n",
      "stages.0.blocks.1.transformer.mlp.fc1.weight: torch.Size([96, 96])\n",
      "stages.0.blocks.1.transformer.mlp.fc1.bias: torch.Size([96])\n",
      "stages.0.blocks.1.transformer.mlp.fc2.weight: torch.Size([96, 96])\n",
      "stages.0.blocks.1.transformer.mlp.fc2.bias: torch.Size([96])\n",
      "stages.1.blocks.0.downsample.reduction.weight: torch.Size([192, 96, 7, 1])\n",
      "stages.1.blocks.0.downsample.reduction.bias: torch.Size([192])\n",
      "stages.1.blocks.0.downsample.bn.weight: torch.Size([192])\n",
      "stages.1.blocks.0.downsample.bn.bias: torch.Size([192])\n",
      "stages.1.blocks.0.downsample.bn.running_mean: torch.Size([192])\n",
      "stages.1.blocks.0.downsample.bn.running_var: torch.Size([192])\n",
      "stages.1.blocks.0.downsample.bn.num_batches_tracked: torch.Size([])\n",
      "stages.1.blocks.0.transformer.gconv: torch.Size([8, 48, 48])\n",
      "stages.1.blocks.0.transformer.norm_1.weight: torch.Size([192])\n",
      "stages.1.blocks.0.transformer.norm_1.bias: torch.Size([192])\n",
      "stages.1.blocks.0.transformer.mapping.weight: torch.Size([384, 192])\n",
      "stages.1.blocks.0.transformer.mapping.bias: torch.Size([384])\n",
      "stages.1.blocks.0.transformer.tconv.weight: torch.Size([48, 6, 7, 1])\n",
      "stages.1.blocks.0.transformer.tconv.bias: torch.Size([48])\n",
      "stages.1.blocks.0.transformer.attention.0.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.1.blocks.0.transformer.attention.0.relative_position_index: torch.Size([8, 8])\n",
      "stages.1.blocks.0.transformer.attention.1.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.1.blocks.0.transformer.attention.1.relative_position_index: torch.Size([8, 8])\n",
      "stages.1.blocks.0.transformer.attention.2.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.1.blocks.0.transformer.attention.2.relative_position_index: torch.Size([8, 8])\n",
      "stages.1.blocks.0.transformer.attention.3.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.1.blocks.0.transformer.attention.3.relative_position_index: torch.Size([8, 8])\n",
      "stages.1.blocks.0.transformer.proj.weight: torch.Size([192, 192])\n",
      "stages.1.blocks.0.transformer.proj.bias: torch.Size([192])\n",
      "stages.1.blocks.0.transformer.norm_2.weight: torch.Size([192])\n",
      "stages.1.blocks.0.transformer.norm_2.bias: torch.Size([192])\n",
      "stages.1.blocks.0.transformer.mlp.fc1.weight: torch.Size([192, 192])\n",
      "stages.1.blocks.0.transformer.mlp.fc1.bias: torch.Size([192])\n",
      "stages.1.blocks.0.transformer.mlp.fc2.weight: torch.Size([192, 192])\n",
      "stages.1.blocks.0.transformer.mlp.fc2.bias: torch.Size([192])\n",
      "stages.1.blocks.1.transformer.gconv: torch.Size([8, 48, 48])\n",
      "stages.1.blocks.1.transformer.norm_1.weight: torch.Size([192])\n",
      "stages.1.blocks.1.transformer.norm_1.bias: torch.Size([192])\n",
      "stages.1.blocks.1.transformer.mapping.weight: torch.Size([384, 192])\n",
      "stages.1.blocks.1.transformer.mapping.bias: torch.Size([384])\n",
      "stages.1.blocks.1.transformer.tconv.weight: torch.Size([48, 6, 7, 1])\n",
      "stages.1.blocks.1.transformer.tconv.bias: torch.Size([48])\n",
      "stages.1.blocks.1.transformer.attention.0.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.1.blocks.1.transformer.attention.0.relative_position_index: torch.Size([8, 8])\n",
      "stages.1.blocks.1.transformer.attention.1.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.1.blocks.1.transformer.attention.1.relative_position_index: torch.Size([8, 8])\n",
      "stages.1.blocks.1.transformer.attention.2.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.1.blocks.1.transformer.attention.2.relative_position_index: torch.Size([8, 8])\n",
      "stages.1.blocks.1.transformer.attention.3.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.1.blocks.1.transformer.attention.3.relative_position_index: torch.Size([8, 8])\n",
      "stages.1.blocks.1.transformer.proj.weight: torch.Size([192, 192])\n",
      "stages.1.blocks.1.transformer.proj.bias: torch.Size([192])\n",
      "stages.1.blocks.1.transformer.norm_2.weight: torch.Size([192])\n",
      "stages.1.blocks.1.transformer.norm_2.bias: torch.Size([192])\n",
      "stages.1.blocks.1.transformer.mlp.fc1.weight: torch.Size([192, 192])\n",
      "stages.1.blocks.1.transformer.mlp.fc1.bias: torch.Size([192])\n",
      "stages.1.blocks.1.transformer.mlp.fc2.weight: torch.Size([192, 192])\n",
      "stages.1.blocks.1.transformer.mlp.fc2.bias: torch.Size([192])\n",
      "stages.2.blocks.0.downsample.reduction.weight: torch.Size([192, 192, 7, 1])\n",
      "stages.2.blocks.0.downsample.reduction.bias: torch.Size([192])\n",
      "stages.2.blocks.0.downsample.bn.weight: torch.Size([192])\n",
      "stages.2.blocks.0.downsample.bn.bias: torch.Size([192])\n",
      "stages.2.blocks.0.downsample.bn.running_mean: torch.Size([192])\n",
      "stages.2.blocks.0.downsample.bn.running_var: torch.Size([192])\n",
      "stages.2.blocks.0.downsample.bn.num_batches_tracked: torch.Size([])\n",
      "stages.2.blocks.0.transformer.gconv: torch.Size([8, 48, 48])\n",
      "stages.2.blocks.0.transformer.norm_1.weight: torch.Size([192])\n",
      "stages.2.blocks.0.transformer.norm_1.bias: torch.Size([192])\n",
      "stages.2.blocks.0.transformer.mapping.weight: torch.Size([384, 192])\n",
      "stages.2.blocks.0.transformer.mapping.bias: torch.Size([384])\n",
      "stages.2.blocks.0.transformer.tconv.weight: torch.Size([48, 6, 7, 1])\n",
      "stages.2.blocks.0.transformer.tconv.bias: torch.Size([48])\n",
      "stages.2.blocks.0.transformer.attention.0.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.2.blocks.0.transformer.attention.0.relative_position_index: torch.Size([8, 8])\n",
      "stages.2.blocks.0.transformer.attention.1.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.2.blocks.0.transformer.attention.1.relative_position_index: torch.Size([8, 8])\n",
      "stages.2.blocks.0.transformer.attention.2.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.2.blocks.0.transformer.attention.2.relative_position_index: torch.Size([8, 8])\n",
      "stages.2.blocks.0.transformer.attention.3.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.2.blocks.0.transformer.attention.3.relative_position_index: torch.Size([8, 8])\n",
      "stages.2.blocks.0.transformer.proj.weight: torch.Size([192, 192])\n",
      "stages.2.blocks.0.transformer.proj.bias: torch.Size([192])\n",
      "stages.2.blocks.0.transformer.norm_2.weight: torch.Size([192])\n",
      "stages.2.blocks.0.transformer.norm_2.bias: torch.Size([192])\n",
      "stages.2.blocks.0.transformer.mlp.fc1.weight: torch.Size([192, 192])\n",
      "stages.2.blocks.0.transformer.mlp.fc1.bias: torch.Size([192])\n",
      "stages.2.blocks.0.transformer.mlp.fc2.weight: torch.Size([192, 192])\n",
      "stages.2.blocks.0.transformer.mlp.fc2.bias: torch.Size([192])\n",
      "stages.2.blocks.1.transformer.gconv: torch.Size([8, 48, 48])\n",
      "stages.2.blocks.1.transformer.norm_1.weight: torch.Size([192])\n",
      "stages.2.blocks.1.transformer.norm_1.bias: torch.Size([192])\n",
      "stages.2.blocks.1.transformer.mapping.weight: torch.Size([384, 192])\n",
      "stages.2.blocks.1.transformer.mapping.bias: torch.Size([384])\n",
      "stages.2.blocks.1.transformer.tconv.weight: torch.Size([48, 6, 7, 1])\n",
      "stages.2.blocks.1.transformer.tconv.bias: torch.Size([48])\n",
      "stages.2.blocks.1.transformer.attention.0.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.2.blocks.1.transformer.attention.0.relative_position_index: torch.Size([8, 8])\n",
      "stages.2.blocks.1.transformer.attention.1.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.2.blocks.1.transformer.attention.1.relative_position_index: torch.Size([8, 8])\n",
      "stages.2.blocks.1.transformer.attention.2.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.2.blocks.1.transformer.attention.2.relative_position_index: torch.Size([8, 8])\n",
      "stages.2.blocks.1.transformer.attention.3.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.2.blocks.1.transformer.attention.3.relative_position_index: torch.Size([8, 8])\n",
      "stages.2.blocks.1.transformer.proj.weight: torch.Size([192, 192])\n",
      "stages.2.blocks.1.transformer.proj.bias: torch.Size([192])\n",
      "stages.2.blocks.1.transformer.norm_2.weight: torch.Size([192])\n",
      "stages.2.blocks.1.transformer.norm_2.bias: torch.Size([192])\n",
      "stages.2.blocks.1.transformer.mlp.fc1.weight: torch.Size([192, 192])\n",
      "stages.2.blocks.1.transformer.mlp.fc1.bias: torch.Size([192])\n",
      "stages.2.blocks.1.transformer.mlp.fc2.weight: torch.Size([192, 192])\n",
      "stages.2.blocks.1.transformer.mlp.fc2.bias: torch.Size([192])\n",
      "stages.3.blocks.0.downsample.reduction.weight: torch.Size([192, 192, 7, 1])\n",
      "stages.3.blocks.0.downsample.reduction.bias: torch.Size([192])\n",
      "stages.3.blocks.0.downsample.bn.weight: torch.Size([192])\n",
      "stages.3.blocks.0.downsample.bn.bias: torch.Size([192])\n",
      "stages.3.blocks.0.downsample.bn.running_mean: torch.Size([192])\n",
      "stages.3.blocks.0.downsample.bn.running_var: torch.Size([192])\n",
      "stages.3.blocks.0.downsample.bn.num_batches_tracked: torch.Size([])\n",
      "stages.3.blocks.0.transformer.gconv: torch.Size([8, 48, 48])\n",
      "stages.3.blocks.0.transformer.norm_1.weight: torch.Size([192])\n",
      "stages.3.blocks.0.transformer.norm_1.bias: torch.Size([192])\n",
      "stages.3.blocks.0.transformer.mapping.weight: torch.Size([384, 192])\n",
      "stages.3.blocks.0.transformer.mapping.bias: torch.Size([384])\n",
      "stages.3.blocks.0.transformer.tconv.weight: torch.Size([48, 6, 7, 1])\n",
      "stages.3.blocks.0.transformer.tconv.bias: torch.Size([48])\n",
      "stages.3.blocks.0.transformer.attention.0.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.3.blocks.0.transformer.attention.0.relative_position_index: torch.Size([8, 8])\n",
      "stages.3.blocks.0.transformer.attention.1.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.3.blocks.0.transformer.attention.1.relative_position_index: torch.Size([8, 8])\n",
      "stages.3.blocks.0.transformer.attention.2.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.3.blocks.0.transformer.attention.2.relative_position_index: torch.Size([8, 8])\n",
      "stages.3.blocks.0.transformer.attention.3.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.3.blocks.0.transformer.attention.3.relative_position_index: torch.Size([8, 8])\n",
      "stages.3.blocks.0.transformer.proj.weight: torch.Size([192, 192])\n",
      "stages.3.blocks.0.transformer.proj.bias: torch.Size([192])\n",
      "stages.3.blocks.0.transformer.norm_2.weight: torch.Size([192])\n",
      "stages.3.blocks.0.transformer.norm_2.bias: torch.Size([192])\n",
      "stages.3.blocks.0.transformer.mlp.fc1.weight: torch.Size([192, 192])\n",
      "stages.3.blocks.0.transformer.mlp.fc1.bias: torch.Size([192])\n",
      "stages.3.blocks.0.transformer.mlp.fc2.weight: torch.Size([192, 192])\n",
      "stages.3.blocks.0.transformer.mlp.fc2.bias: torch.Size([192])\n",
      "stages.3.blocks.1.transformer.gconv: torch.Size([8, 48, 48])\n",
      "stages.3.blocks.1.transformer.norm_1.weight: torch.Size([192])\n",
      "stages.3.blocks.1.transformer.norm_1.bias: torch.Size([192])\n",
      "stages.3.blocks.1.transformer.mapping.weight: torch.Size([384, 192])\n",
      "stages.3.blocks.1.transformer.mapping.bias: torch.Size([384])\n",
      "stages.3.blocks.1.transformer.tconv.weight: torch.Size([48, 6, 7, 1])\n",
      "stages.3.blocks.1.transformer.tconv.bias: torch.Size([48])\n",
      "stages.3.blocks.1.transformer.attention.0.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.3.blocks.1.transformer.attention.0.relative_position_index: torch.Size([8, 8])\n",
      "stages.3.blocks.1.transformer.attention.1.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.3.blocks.1.transformer.attention.1.relative_position_index: torch.Size([8, 8])\n",
      "stages.3.blocks.1.transformer.attention.2.relative_position_bias_table: torch.Size([15, 4])\n",
      "stages.3.blocks.1.transformer.attention.2.relative_position_index: torch.Size([8, 8])\n",
      "stages.3.blocks.1.transformer.attention.3.relative_position_bias_table: torch.Size([15, 24, 24, 4])\n",
      "stages.3.blocks.1.transformer.attention.3.relative_position_index: torch.Size([8, 8])\n",
      "stages.3.blocks.1.transformer.proj.weight: torch.Size([192, 192])\n",
      "stages.3.blocks.1.transformer.proj.bias: torch.Size([192])\n",
      "stages.3.blocks.1.transformer.norm_2.weight: torch.Size([192])\n",
      "stages.3.blocks.1.transformer.norm_2.bias: torch.Size([192])\n",
      "stages.3.blocks.1.transformer.mlp.fc1.weight: torch.Size([192, 192])\n",
      "stages.3.blocks.1.transformer.mlp.fc1.bias: torch.Size([192])\n",
      "stages.3.blocks.1.transformer.mlp.fc2.weight: torch.Size([192, 192])\n",
      "stages.3.blocks.1.transformer.mlp.fc2.bias: torch.Size([192])\n",
      "head.weight: torch.Size([52, 192])\n",
      "head.bias: torch.Size([52])\n",
      "projection_head.0.weight: torch.Size([192, 192])\n",
      "projection_head.0.bias: torch.Size([192])\n",
      "projection_head.2.weight: torch.Size([128, 192])\n",
      "projection_head.2.bias: torch.Size([128])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T17:28:10.855879Z",
     "start_time": "2025-02-22T17:28:10.847906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "RH = 94\n",
    "LH = 115\n",
    "FK = 26\n",
    "# 定义骨骼连接关系，列表中的元组表示骨骼的连接点\n",
    "bone = [\n",
    "    # Head\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    # Body\n",
    "    (0, 18),\n",
    "    (5, 18), (6, 18), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "    (17, 18), (18, 19), (19, 11), (19, 12),\n",
    "    (11, 13), (12, 14), (13, 15), (14, 16),\n",
    "    # Foot\n",
    "    (20, 24), (21, 25), (23, 25), (22, 24), (15, 24), (16, 25),\n",
    "    # Right Hand\n",
    "    (9, RH+0), (RH+0, RH+4), (RH+0, RH+8), (RH+0, RH+8), (RH+0, RH+12), (RH+0, RH+16), (RH+0, RH+20),\n",
    "    # Left Hand\n",
    "    (10, LH + 0), (LH + 0, LH + 4), (LH + 0, LH + 8), (LH + 0, LH + 12), (LH + 0, LH + 16), (LH + 0, LH + 20),\n",
    "    # Face\n",
    "    (FK + 33, 0), (FK + 51, FK + 57), (FK + 33, FK + 51), (FK + 28, FK + 33), (FK + 28, FK + 45), (FK + 28, FK + 36)\n",
    "]\n",
    "\n",
    "right_hand = np.array([RH+0, RH+4, RH+8, RH+12, RH+16, RH+20])\n",
    "left_hand = np.array([LH+0, LH+4, LH+8, LH+12, LH+16, LH+20])\n",
    "\n",
    "right_arm = np.array([5,5,7,7,9,9])\n",
    "left_arm = np.array([6,6,8,8,10,10])\n",
    "\n",
    "right_leg = np.array([11,13,15,24,22,20])\n",
    "left_leg = np.array([12,14,16,25,23,21])\n",
    "\n",
    "face = np.array([FK+57, FK+51, FK+33, FK+28, FK+45, FK+36])\n",
    "torso = np.array([18,0,3,4,17,19])\n",
    "\n",
    "new_idx = np.concatenate(\n",
    "    (right_hand, left_hand, right_arm, left_arm, right_leg, left_leg, face, torso), axis=-1)\n"
   ],
   "id": "c17a7b90914c5542",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T17:29:29.616087Z",
     "start_time": "2025-02-22T17:29:29.610760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dic = dict()\n",
    "for i, n in enumerate(list(new_idx)):\n",
    "    dic[n] = i"
   ],
   "id": "95788c5fc799d2f2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T17:29:49.966610Z",
     "start_time": "2025-02-22T17:29:49.953653Z"
    }
   },
   "cell_type": "code",
   "source": "dic",
   "id": "39d3ca48a63af9c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{94: 0,\n",
       " 98: 1,\n",
       " 102: 2,\n",
       " 106: 3,\n",
       " 110: 4,\n",
       " 114: 5,\n",
       " 115: 6,\n",
       " 119: 7,\n",
       " 123: 8,\n",
       " 127: 9,\n",
       " 131: 10,\n",
       " 135: 11,\n",
       " 5: 13,\n",
       " 7: 15,\n",
       " 9: 17,\n",
       " 6: 19,\n",
       " 8: 21,\n",
       " 10: 23,\n",
       " 11: 24,\n",
       " 13: 25,\n",
       " 15: 26,\n",
       " 24: 27,\n",
       " 22: 28,\n",
       " 20: 29,\n",
       " 12: 30,\n",
       " 14: 31,\n",
       " 16: 32,\n",
       " 25: 33,\n",
       " 23: 34,\n",
       " 21: 35,\n",
       " 83: 36,\n",
       " 77: 37,\n",
       " 59: 38,\n",
       " 54: 39,\n",
       " 71: 40,\n",
       " 62: 41,\n",
       " 18: 42,\n",
       " 0: 43,\n",
       " 3: 44,\n",
       " 4: 45,\n",
       " 17: 46,\n",
       " 19: 47}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T17:33:05.665109Z",
     "start_time": "2025-02-22T17:33:05.652025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "O = []\n",
    "for i in bone:\n",
    "    print(i)\n",
    "    if i[0] not in dic.keys() or i[1] not in dic.keys():\n",
    "        continue\n",
    "    O.append((dic[i[0]], dic[i[1]]))"
   ],
   "id": "5178ce6941fa046c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "(0, 2)\n",
      "(1, 3)\n",
      "(2, 4)\n",
      "(0, 18)\n",
      "(5, 18)\n",
      "(6, 18)\n",
      "(5, 7)\n",
      "(7, 9)\n",
      "(6, 8)\n",
      "(8, 10)\n",
      "(17, 18)\n",
      "(18, 19)\n",
      "(19, 11)\n",
      "(19, 12)\n",
      "(11, 13)\n",
      "(12, 14)\n",
      "(13, 15)\n",
      "(14, 16)\n",
      "(20, 24)\n",
      "(21, 25)\n",
      "(23, 25)\n",
      "(22, 24)\n",
      "(15, 24)\n",
      "(16, 25)\n",
      "(9, 94)\n",
      "(94, 98)\n",
      "(94, 102)\n",
      "(94, 102)\n",
      "(94, 106)\n",
      "(94, 110)\n",
      "(94, 114)\n",
      "(10, 115)\n",
      "(115, 119)\n",
      "(115, 123)\n",
      "(115, 127)\n",
      "(115, 131)\n",
      "(115, 135)\n",
      "(59, 0)\n",
      "(77, 83)\n",
      "(59, 77)\n",
      "(54, 59)\n",
      "(54, 71)\n",
      "(54, 62)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T17:33:21.366793Z",
     "start_time": "2025-02-22T17:33:21.353600Z"
    }
   },
   "cell_type": "code",
   "source": "O",
   "id": "b2d761a279db4326",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(43, 42),\n",
       " (13, 42),\n",
       " (19, 42),\n",
       " (13, 15),\n",
       " (15, 17),\n",
       " (19, 21),\n",
       " (21, 23),\n",
       " (46, 42),\n",
       " (42, 47),\n",
       " (47, 24),\n",
       " (47, 30),\n",
       " (24, 25),\n",
       " (30, 31),\n",
       " (25, 26),\n",
       " (31, 32),\n",
       " (29, 27),\n",
       " (35, 33),\n",
       " (34, 33),\n",
       " (28, 27),\n",
       " (26, 27),\n",
       " (32, 33),\n",
       " (17, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (0, 5),\n",
       " (23, 6),\n",
       " (6, 7),\n",
       " (6, 8),\n",
       " (6, 9),\n",
       " (6, 10),\n",
       " (6, 11),\n",
       " (38, 43),\n",
       " (37, 36),\n",
       " (38, 37),\n",
       " (39, 38),\n",
       " (39, 40),\n",
       " (39, 41)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
